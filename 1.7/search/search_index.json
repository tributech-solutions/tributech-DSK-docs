{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"index.html","text":"","title":"Home"},{"location":"assets/iot-hub-arm-template/index.html","text":"IoT Hub Template \u00b6 The Template in this folder is used for the quickstart guide of the documentation. It is a slight modification of the template in the Azure DevOps repository. If you updated the Template in the repo for some reason, make sure to update this copy of the template as well. Modifications \u00b6 Default to IoT Hub Tier: 'S1' Default ID-Server to: 'https://id.azuretrial.dataspace-hub.com' Removed input for ... used value ... instead Docer Registry => 'pubtributech.azurecr.io' Image Name => 'dsk-iot-hub-bridge' Registry User => 'pubtributech' Registry Password => registry password","title":"IoT Hub Template"},{"location":"assets/iot-hub-arm-template/index.html#iot-hub-template","text":"The Template in this folder is used for the quickstart guide of the documentation. It is a slight modification of the template in the Azure DevOps repository. If you updated the Template in the repo for some reason, make sure to update this copy of the template as well.","title":"IoT Hub Template"},{"location":"assets/iot-hub-arm-template/index.html#modifications","text":"Default to IoT Hub Tier: 'S1' Default ID-Server to: 'https://id.azuretrial.dataspace-hub.com' Removed input for ... used value ... instead Docer Registry => 'pubtributech.azurecr.io' Image Name => 'dsk-iot-hub-bridge' Registry User => 'pubtributech' Registry Password => registry password","title":"Modifications"},{"location":"css/index.html","text":"mkdocs-material-dark-theme \u00b6 External SCSS/CSS file that can change the appearance of mkdocs-material theme and render it in dark colors. To have proper syntax highlighting, don't forget to add this code to your mkdocs.yml file: markdown_extensions : - codehilite : guess_lang : False use_pygments : True noclasses : True pygments_style : monokai","title":"mkdocs-material-dark-theme"},{"location":"css/index.html#mkdocs-material-dark-theme","text":"External SCSS/CSS file that can change the appearance of mkdocs-material theme and render it in dark colors. To have proper syntax highlighting, don't forget to add this code to your mkdocs.yml file: markdown_extensions : - codehilite : guess_lang : False use_pygments : True noclasses : True pygments_style : monokai","title":"mkdocs-material-dark-theme"},{"location":"integration/overview.html","text":"Integration \u00b6 The Tributech DataSpace Kit can be integrated on many levels, for which an overview is given here. DataSpace Kit Node \u00b6 The DSK Node provides a set of REST APIs to access data as well as verify data authenticity and data integrity. Learn more about how to use them in our node integration guide . DataSpace Kit Agent \u00b6 The DSK Agent can be integrated depending on the variant that you are working with: DSK Agent Edge DSK Agent Embedded DSK Agent Integrated Read more about integration details for the DSK Agent Edge here . The DSK Agent Integrated can be used through the Trust-Api - please read about how to use it at the node integration guide .","title":"Overview"},{"location":"integration/overview.html#integration","text":"The Tributech DataSpace Kit can be integrated on many levels, for which an overview is given here.","title":"Integration"},{"location":"integration/overview.html#dataspace-kit-node","text":"The DSK Node provides a set of REST APIs to access data as well as verify data authenticity and data integrity. Learn more about how to use them in our node integration guide .","title":"DataSpace Kit Node"},{"location":"integration/overview.html#dataspace-kit-agent","text":"The DSK Agent can be integrated depending on the variant that you are working with: DSK Agent Edge DSK Agent Embedded DSK Agent Integrated Read more about integration details for the DSK Agent Edge here . The DSK Agent Integrated can be used through the Trust-Api - please read about how to use it at the node integration guide .","title":"DataSpace Kit Agent"},{"location":"integration/agent/ssm.html","text":"","title":"Ssm"},{"location":"integration/agent/edge/configuration.html","text":"DSK Agent Edge Configuration Options \u00b6 This page gives an overview of the available Configuration Options for the DSK Agent Edge. Common Options \u00b6 Config Option Expected Value Sample value Logging__LogLevel__Default Log level of the complete application LogLevel MQTT Options \u00b6 Config Option Expected Value Sample value MqttOptions__MQTTHost MQTT server adress localhost MqttOptions__MQTTPort MQTT server port 1883 MqttOptions__MQTTUser MQTT username user MqttOptions__MQTTPassword MQTT password password MqttOptions__ConnectionTimeout MQTT connection timeout in ms 30000 Agent Options \u00b6 Config Option Expected Value Sample value SignerOptions__PrivateKeyFilePath Private key file path /path/to/private-key.pem SignerOptions__PublicKeyFilePath Public key file path /path/to/public-key.pem EdgeDeviceOptions__AgentID ID Should be uniqe for each agent 51319195 EdgeDeviceOptions__DefaultMaxMerkleTreeDepth Default maximal merkle tree depth. This value is overridden by stream-specific coptions. 12 EdgeDeviceOptions__DefaultMaxMerkleTreeAge Default maximal merkle-tree age in seconds. This value is overridden by stream-specific coptions. 600 EdgeDeviceOptions__DataStreamOptions__<ValueMetadataID>__MaxMerkleTreeDepth Stream specific maximal merkle-tree depth. EdgeDeviceOptions__DataStreamOptions__ea6eb3e0-1280-4b36-a807-94133cd7c775__MaxMerkleTreeDepth=24 EdgeDeviceOptions__DataStreamOptions__<ValueMetadataID>__MaxMerkleTreeAge Stream specific maximal merkle-tree age. EdgeDeviceOptions__DataStreamOptions__ea6eb3e0-1280-4b36-a807-94133cd7c775__MaxMerkleTreeDepth=900 DataStreamID__<ValueMetadataID> <ValueSourceType>__<ValueSourceID>__<SensorID> DataStreamID__ea6eb3e0-1280-4b36-a807-94133cd7c775=OpcuaSource__opc.tcp://127.0.0.1:4840/va_tt_demo__ns=2;i=3 Azure IoT Hub ValueSink Options \u00b6 Config Option Expected Value Sample value AzureIoTHubOptions__ConnectionString Connection string for the IoT Hub Edge Device (Important: Not Edge Runtime Device!) HostName=your-iot-hub.azure-devices.net;DeviceId=MyDotnetTestDevice;SharedAccessKey=IfBQvqDPKffysR0e0Do85uw6aaY/w3IFTnxNK3BoGGk= IoTHubValueSinkOptions__MaxBatchSize Maximum number of values that are batched together and sent to the IoT Hub 500 IoTHubValueSinkOptions__FlushInterval Maximum timer-interval of a single batch of values that is sent to the IoT Hub 00:00:01.000 Proof Sink Options \u00b6 Config Option Expected Value Sample value ProofSinkOptions__TrustAPIBaseUrl Url of the Trust API https://trust-api.your-node.dataspace-node.com/ ProofSinkOptions__AuthUrl Url of the identity-server https://id.your-hub.dataspace-hub.com/connect/token ProofSinkOptions__AuthScope OAuth scope trust-api-endpoint data-api-endpoint ProofSinkOptions__ClientID OAuth client-id 7f89e8f9-4ecc-434e-a674-bfe48912aa56 ProofSinkOptions__ClientSecret OAuth client-secret some-client-secret123 ProofSinkOptions__MaxBatchSize Maximum number of proofs that are batched together 20 ProofSinkOptions__FlushInterval Maximum time-interval of a single batch of proofs 00:00:01.000 ProofSinkOptions__TrustApiMaxRetries Maximum number of retries if Trust API cannot be reached 3 ProofSinkOptions__TrustApiMaxRetryInterval Maximum interval betwenn retries. Retries use exponential backoff with this maximal interval. 00:05:00.000","title":"Configuration"},{"location":"integration/agent/edge/configuration.html#dsk-agent-edge-configuration-options","text":"This page gives an overview of the available Configuration Options for the DSK Agent Edge.","title":"DSK Agent Edge Configuration Options"},{"location":"integration/agent/edge/configuration.html#common-options","text":"Config Option Expected Value Sample value Logging__LogLevel__Default Log level of the complete application LogLevel","title":"Common Options"},{"location":"integration/agent/edge/configuration.html#mqtt-options","text":"Config Option Expected Value Sample value MqttOptions__MQTTHost MQTT server adress localhost MqttOptions__MQTTPort MQTT server port 1883 MqttOptions__MQTTUser MQTT username user MqttOptions__MQTTPassword MQTT password password MqttOptions__ConnectionTimeout MQTT connection timeout in ms 30000","title":"MQTT Options"},{"location":"integration/agent/edge/configuration.html#agent-options","text":"Config Option Expected Value Sample value SignerOptions__PrivateKeyFilePath Private key file path /path/to/private-key.pem SignerOptions__PublicKeyFilePath Public key file path /path/to/public-key.pem EdgeDeviceOptions__AgentID ID Should be uniqe for each agent 51319195 EdgeDeviceOptions__DefaultMaxMerkleTreeDepth Default maximal merkle tree depth. This value is overridden by stream-specific coptions. 12 EdgeDeviceOptions__DefaultMaxMerkleTreeAge Default maximal merkle-tree age in seconds. This value is overridden by stream-specific coptions. 600 EdgeDeviceOptions__DataStreamOptions__<ValueMetadataID>__MaxMerkleTreeDepth Stream specific maximal merkle-tree depth. EdgeDeviceOptions__DataStreamOptions__ea6eb3e0-1280-4b36-a807-94133cd7c775__MaxMerkleTreeDepth=24 EdgeDeviceOptions__DataStreamOptions__<ValueMetadataID>__MaxMerkleTreeAge Stream specific maximal merkle-tree age. EdgeDeviceOptions__DataStreamOptions__ea6eb3e0-1280-4b36-a807-94133cd7c775__MaxMerkleTreeDepth=900 DataStreamID__<ValueMetadataID> <ValueSourceType>__<ValueSourceID>__<SensorID> DataStreamID__ea6eb3e0-1280-4b36-a807-94133cd7c775=OpcuaSource__opc.tcp://127.0.0.1:4840/va_tt_demo__ns=2;i=3","title":"Agent Options"},{"location":"integration/agent/edge/configuration.html#azure-iot-hub-valuesink-options","text":"Config Option Expected Value Sample value AzureIoTHubOptions__ConnectionString Connection string for the IoT Hub Edge Device (Important: Not Edge Runtime Device!) HostName=your-iot-hub.azure-devices.net;DeviceId=MyDotnetTestDevice;SharedAccessKey=IfBQvqDPKffysR0e0Do85uw6aaY/w3IFTnxNK3BoGGk= IoTHubValueSinkOptions__MaxBatchSize Maximum number of values that are batched together and sent to the IoT Hub 500 IoTHubValueSinkOptions__FlushInterval Maximum timer-interval of a single batch of values that is sent to the IoT Hub 00:00:01.000","title":"Azure IoT Hub ValueSink Options"},{"location":"integration/agent/edge/configuration.html#proof-sink-options","text":"Config Option Expected Value Sample value ProofSinkOptions__TrustAPIBaseUrl Url of the Trust API https://trust-api.your-node.dataspace-node.com/ ProofSinkOptions__AuthUrl Url of the identity-server https://id.your-hub.dataspace-hub.com/connect/token ProofSinkOptions__AuthScope OAuth scope trust-api-endpoint data-api-endpoint ProofSinkOptions__ClientID OAuth client-id 7f89e8f9-4ecc-434e-a674-bfe48912aa56 ProofSinkOptions__ClientSecret OAuth client-secret some-client-secret123 ProofSinkOptions__MaxBatchSize Maximum number of proofs that are batched together 20 ProofSinkOptions__FlushInterval Maximum time-interval of a single batch of proofs 00:00:01.000 ProofSinkOptions__TrustApiMaxRetries Maximum number of retries if Trust API cannot be reached 3 ProofSinkOptions__TrustApiMaxRetryInterval Maximum interval betwenn retries. Retries use exponential backoff with this maximal interval. 00:05:00.000","title":"Proof Sink Options"},{"location":"integration/agent/edge/integration.html","text":"DSK Agent Edge Integration \u00b6 Integration \u00b6 Integration of the DSK Agent Edge is simple since you can use your default exisiting connectors. This is due to the fact, that the agent is integrated at the level of the message bus. MQTT is supported by standard, hence the Agent is compatible with all major message brokers & IoT Runtime solutions. It is e.g. tested and deployed in production with Mosquitto and RabbitMQ. Conclusively, you can use your exisiting connectors, all you have to do is adapt the configuration / routing. Configuration Options for the DSK Agent Edge can be found here . Next Steps \u00b6 Read about DSK Node integration","title":"Integration"},{"location":"integration/agent/edge/integration.html#dsk-agent-edge-integration","text":"","title":"DSK Agent Edge Integration"},{"location":"integration/agent/edge/integration.html#integration","text":"Integration of the DSK Agent Edge is simple since you can use your default exisiting connectors. This is due to the fact, that the agent is integrated at the level of the message bus. MQTT is supported by standard, hence the Agent is compatible with all major message brokers & IoT Runtime solutions. It is e.g. tested and deployed in production with Mosquitto and RabbitMQ. Conclusively, you can use your exisiting connectors, all you have to do is adapt the configuration / routing. Configuration Options for the DSK Agent Edge can be found here .","title":"Integration"},{"location":"integration/agent/edge/integration.html#next-steps","text":"Read about DSK Node integration","title":"Next Steps"},{"location":"integration/node/integration-examples.html","text":"Integration Examples \u00b6 The simplest way to integrate our APIs is through the offered nuget package . For easy integration of the Data API and Trust API we also offer a public GitHub repository which contains our API clients and code-samples showcasing how the APIs can be used. If you do not use C# you can generate your own API Client using any one of the publicly available OpenAPI code generators, such as the OpenAPI Generator . Next steps \u00b6 Read about Agent Edge Integration Check out the API-Clients and code samples API Clients","title":"Integration Examples"},{"location":"integration/node/integration-examples.html#integration-examples","text":"The simplest way to integrate our APIs is through the offered nuget package . For easy integration of the Data API and Trust API we also offer a public GitHub repository which contains our API clients and code-samples showcasing how the APIs can be used. If you do not use C# you can generate your own API Client using any one of the publicly available OpenAPI code generators, such as the OpenAPI Generator .","title":"Integration Examples"},{"location":"integration/node/integration-examples.html#next-steps","text":"Read about Agent Edge Integration Check out the API-Clients and code samples API Clients","title":"Next steps"},{"location":"integration/node/overview.html","text":"DSK Node Integration Overview \u00b6 The DSK Node offers two APIs which are used for integration: the Data API (values) and the Trust API (proofs). All of the available endpoints usually work with the UUID of a data stream (ValueMetadataId + Timestamp). Both of them can be easily integrated through our ready-to-use solutions explained here . Trust API \u00b6 The Trust API provides the interface to consume/store proofs used for data auditability. This API is offered for Agents to submit their proofs. The Trust API also comes with an integrated Agent which can be used to store values and create the according proofs - the DSK Agent Integrated. Furthermore the Trust API also contains endpoints to validate the auditability of data. Data API \u00b6 The Data Api is used to consume/store the actual values of a data stream. Swagger UI \u00b6 Both the Trust API and the Data API come with a Swagger UI. Please follow our guide for accessing the instance of your node or alternatively if you just want to quickly check out the available endpoints then you can have a look at our public demo instances here: Trust API Data API Next steps \u00b6 Check out our Integration Examples Learn about Agent Edge Integration","title":"Overview"},{"location":"integration/node/overview.html#dsk-node-integration-overview","text":"The DSK Node offers two APIs which are used for integration: the Data API (values) and the Trust API (proofs). All of the available endpoints usually work with the UUID of a data stream (ValueMetadataId + Timestamp). Both of them can be easily integrated through our ready-to-use solutions explained here .","title":"DSK Node Integration Overview"},{"location":"integration/node/overview.html#trust-api","text":"The Trust API provides the interface to consume/store proofs used for data auditability. This API is offered for Agents to submit their proofs. The Trust API also comes with an integrated Agent which can be used to store values and create the according proofs - the DSK Agent Integrated. Furthermore the Trust API also contains endpoints to validate the auditability of data.","title":"Trust API"},{"location":"integration/node/overview.html#data-api","text":"The Data Api is used to consume/store the actual values of a data stream.","title":"Data API"},{"location":"integration/node/overview.html#swagger-ui","text":"Both the Trust API and the Data API come with a Swagger UI. Please follow our guide for accessing the instance of your node or alternatively if you just want to quickly check out the available endpoints then you can have a look at our public demo instances here: Trust API Data API","title":"Swagger UI"},{"location":"integration/node/overview.html#next-steps","text":"Check out our Integration Examples Learn about Agent Edge Integration","title":"Next steps"},{"location":"integration/node/swagger-ui-authorization.html","text":"Swagger UI Authorization \u00b6 Navigate to the Swagger UI \u00b6 You can easily navigate to the Swagger UI of the API you are interested in through the DataSpace Admin App by clicking on the link in the sidebar menu: Authorization at the Swagger UI \u00b6 Next you have to authorize yourself for usage of the Swagger UI. First open the authorization dialog: You will need the client_ID and client_secret which you can find in the DataSpace Admin App as shown below: Insert those values in the Authorization dialog input fields, toggle the Checkbox for the scopes and click the Authorize button:","title":"Swagger UI Authorization"},{"location":"integration/node/swagger-ui-authorization.html#swagger-ui-authorization","text":"","title":"Swagger UI Authorization"},{"location":"integration/node/swagger-ui-authorization.html#navigate-to-the-swagger-ui","text":"You can easily navigate to the Swagger UI of the API you are interested in through the DataSpace Admin App by clicking on the link in the sidebar menu:","title":"Navigate to the Swagger UI"},{"location":"integration/node/swagger-ui-authorization.html#authorization-at-the-swagger-ui","text":"Next you have to authorize yourself for usage of the Swagger UI. First open the authorization dialog: You will need the client_ID and client_secret which you can find in the DataSpace Admin App as shown below: Insert those values in the Authorization dialog input fields, toggle the Checkbox for the scopes and click the Authorize button:","title":"Authorization at the Swagger UI"},{"location":"introduction/architecture.html","text":"Tributech DataSpace Kit: Architecture \u00b6 As explained in the Product Overview , the three fundamental components for the Tributech DataSpace Kit are the DataSpace Hub, the DataSpace Node and the DataSpace Agent. A closer look at these components is presented below. Fig.1: DataSpace Architecture Representation DataSpace Hub \u00b6 The DataSpace Hub provides the metadata storage and the public key infrastucture that is required for setting up an ecosystem of DataSpace Nodes. In short, the Hub holds all Metadata information required for managing and organization of the secure auditable data sharing between nodes. Note that the Hub does not hold any data itself, thus guaranteeing the data soverignity of the node owners! Thus, the Hub is responsible for: storing the metadata of published datasets adding new DataSpace Nodes to the ecosystem issuing of certificates managing public keys managing identities Note: A DataSpace Hub is not on the route of the traffic for the data exchange and even a man-in-the-middle-attack would fail because Hub has no private-key material of DataSpace Nodes or Agents. DataSpace Node \u00b6 A DataSpace Node is acting as a gateway/broker that is required to participate in a DataSpace Ecosystem. It contains all services needed for the data exchange and data verification capabilities with other nodes inside a DataSpace Ecosystem. Each DataSpace Node is controlled by a person or organization. The DataSpace Node comes with an integrated Trust Layer, which ensures data auditability and guarantees the tamperproof storage of the proofs thorugh a distributed systems technology which shares the proofs across all nodes in the ecosystem. The Dataspace Node comes with a web UI application called DataSpace Admin which allows for easy maintaining of the node and interacting with other members of the ecosystem. The interface of the app can be seen in Fig.1. Fig.2: DataSpace Admin App_ With a DataSpace Node you can: Connect data sources via APIs and combine selected data to datasets. The integrated web UI application DataSpace Admin provides an interface to generate the required metadata that defines a dataset. For each stream (not limited to a data format or type) a unique ID is generated which is used as a reference for the data source integration via the APIs of the Dataspace Node. Through the unique IDs from data streams of a dataset, other participants inside a ecosystem can request and consume data. Share data streams with your customers, suppliers and partners. The DataSpace Node allows every user to create individual and automated synchronization processes to exchange data, without the need to care about networking, data processing, authentication and encryption between the connected systems. The integrated web application covers the data sharing process for datasets via publish, request and grant workflows. Audit data by checking the origin and integrity before you start using it. Data can be audited through the integrated Audit tool of the DataSpace Admin app or through the available API. Ready more about how Tributech guarantees the security for your auditable data here . Track & trace conditions at your data source via a comparable history of configuration changes. Insights from tracked changes provide viable information for e.g. AI/ML applications in order to match anomalies in data with historic configuration changes. Visualize data through the integrated Node dashboards based on the well-known and well-established Open Source technology Grafana . DataSpace Agent \u00b6 The DataSpace Agent is designed for an integration into the data source for providing cross-system data security for any type of device/source like e.g. sensors, IoT-devices, IoT-gateways, SCADA-systems, PPS-systems, ERP-systems and more. A scalable process for generating cryptographic proofs and a secure connection to the trust layer guarantees the raw data\u2019s authenticity and integrity for an entire ecosystem of DataSpace Nodes. Fig.3: DataSpace Agent Schema The DataSpace Agent enriches a data source in terms of security and reliability. In addition to the existing data telemetry, hashes and signatures are created for each data point/packet/portion and these proofs are transmitted via a dedicated authenticated and encrypted communication channel to the trust layer. The DataSpace Kit has a distributed system topology in order to provide secure peer-to-peer connections for the data sharing process and an immutable system for the data verification capabilities. Next steps \u00b6 Set up a demo of the product through our quick start guide Learn more about how to integrate the DataSpace Kit into your existing infrastructure","title":"Architecture"},{"location":"introduction/architecture.html#tributech-dataspace-kit-architecture","text":"As explained in the Product Overview , the three fundamental components for the Tributech DataSpace Kit are the DataSpace Hub, the DataSpace Node and the DataSpace Agent. A closer look at these components is presented below. Fig.1: DataSpace Architecture Representation","title":"Tributech DataSpace Kit: Architecture"},{"location":"introduction/architecture.html#dataspace-hub","text":"The DataSpace Hub provides the metadata storage and the public key infrastucture that is required for setting up an ecosystem of DataSpace Nodes. In short, the Hub holds all Metadata information required for managing and organization of the secure auditable data sharing between nodes. Note that the Hub does not hold any data itself, thus guaranteeing the data soverignity of the node owners! Thus, the Hub is responsible for: storing the metadata of published datasets adding new DataSpace Nodes to the ecosystem issuing of certificates managing public keys managing identities Note: A DataSpace Hub is not on the route of the traffic for the data exchange and even a man-in-the-middle-attack would fail because Hub has no private-key material of DataSpace Nodes or Agents.","title":"DataSpace Hub"},{"location":"introduction/architecture.html#dataspace-node","text":"A DataSpace Node is acting as a gateway/broker that is required to participate in a DataSpace Ecosystem. It contains all services needed for the data exchange and data verification capabilities with other nodes inside a DataSpace Ecosystem. Each DataSpace Node is controlled by a person or organization. The DataSpace Node comes with an integrated Trust Layer, which ensures data auditability and guarantees the tamperproof storage of the proofs thorugh a distributed systems technology which shares the proofs across all nodes in the ecosystem. The Dataspace Node comes with a web UI application called DataSpace Admin which allows for easy maintaining of the node and interacting with other members of the ecosystem. The interface of the app can be seen in Fig.1. Fig.2: DataSpace Admin App_ With a DataSpace Node you can: Connect data sources via APIs and combine selected data to datasets. The integrated web UI application DataSpace Admin provides an interface to generate the required metadata that defines a dataset. For each stream (not limited to a data format or type) a unique ID is generated which is used as a reference for the data source integration via the APIs of the Dataspace Node. Through the unique IDs from data streams of a dataset, other participants inside a ecosystem can request and consume data. Share data streams with your customers, suppliers and partners. The DataSpace Node allows every user to create individual and automated synchronization processes to exchange data, without the need to care about networking, data processing, authentication and encryption between the connected systems. The integrated web application covers the data sharing process for datasets via publish, request and grant workflows. Audit data by checking the origin and integrity before you start using it. Data can be audited through the integrated Audit tool of the DataSpace Admin app or through the available API. Ready more about how Tributech guarantees the security for your auditable data here . Track & trace conditions at your data source via a comparable history of configuration changes. Insights from tracked changes provide viable information for e.g. AI/ML applications in order to match anomalies in data with historic configuration changes. Visualize data through the integrated Node dashboards based on the well-known and well-established Open Source technology Grafana .","title":"DataSpace Node"},{"location":"introduction/architecture.html#dataspace-agent","text":"The DataSpace Agent is designed for an integration into the data source for providing cross-system data security for any type of device/source like e.g. sensors, IoT-devices, IoT-gateways, SCADA-systems, PPS-systems, ERP-systems and more. A scalable process for generating cryptographic proofs and a secure connection to the trust layer guarantees the raw data\u2019s authenticity and integrity for an entire ecosystem of DataSpace Nodes. Fig.3: DataSpace Agent Schema The DataSpace Agent enriches a data source in terms of security and reliability. In addition to the existing data telemetry, hashes and signatures are created for each data point/packet/portion and these proofs are transmitted via a dedicated authenticated and encrypted communication channel to the trust layer. The DataSpace Kit has a distributed system topology in order to provide secure peer-to-peer connections for the data sharing process and an immutable system for the data verification capabilities.","title":"DataSpace Agent"},{"location":"introduction/architecture.html#next-steps","text":"Set up a demo of the product through our quick start guide Learn more about how to integrate the DataSpace Kit into your existing infrastructure","title":"Next steps"},{"location":"introduction/product-overview.html","text":"Tributech DataSpace Kit: Introduction \u00b6 Data exchange reinvented - Secure. Traceable. Trusted. \u00b6 The Tributech DataSpace Kit enables companies to share auditable data cross-company or cross-process in a selective way while maintaining data sovereignty. This data could be sensor-, process- or business data: data sources can be upgraded with data verification capabilities that guarantee the traceability and auditability of consumed and delivered data. In other words, the Tributech DataSpace Kit is a containerized software-tool-kit that covers the whole data transfer between DataSpace Nodes (which usually represents a company's cloud or server infrastructure) and it covers the transfer of cryptographic proofs of the DataSpace Agents which enable data auditability. Based on the containerized and modular architecture, the technology can be integrated into any kind of platform , cloud or on-premise system to support the technology stack of all participating stakeholders. Check out our Integration guide for details. If you want to learn more about use-cases for your business, visit our website , visit our blog or contact our Customer Advisory Team that helps you to get started. The building blocks for a Tributech DataSpace Ecosystem \u00b6 The Tributech DataSpace Kit consists of three building blocks that allow you to create your own DataSpace Ecosystem or to join an existing one. An Ecosystem can represent, for example, a value chain in which customers, suppliers and manufacturers use the DataSpace Kit in order to exchange different kinds of auditable data streams to optimize their supply chain processes. As shown below in Fig.1, these building blocks are: DataSpace Hub DataSpace Node DataSpace Agent Every ecosystem consists of one single DataSpace Hub , a DataSpace Node for each participating stakeholder integrated in his infrastructure (cloud, hybrid or on-premise) and multiple DataSpace Agents integrated into each data-source/device that requires the data auditability capabilities. The Hub holds metadata information and manages identity, required for secure auditable data sharing in the ecosystem - but is not part of the data sharing channels themselves. Actual data is shared only between nodes. The Agents feed auditable data to the nodes. Fig.1: DataSpace Kit Building Blocks & Schema_ The distributed components of the DataSpace Kit are connected via channels to exchange the different kinds of data within an ecosystem. APIs provide an interface for the data integration within the infrastructure of each connected stakeholder. Next steps \u00b6 Learn more about the architecture of the DataSpace Kit and how it can be integrated into your existing infrastructure Set up a demo of the product through our quick start guide","title":"Product Overview"},{"location":"introduction/product-overview.html#tributech-dataspace-kit-introduction","text":"","title":"Tributech DataSpace Kit: Introduction"},{"location":"introduction/product-overview.html#data-exchange-reinvented-secure-traceable-trusted","text":"The Tributech DataSpace Kit enables companies to share auditable data cross-company or cross-process in a selective way while maintaining data sovereignty. This data could be sensor-, process- or business data: data sources can be upgraded with data verification capabilities that guarantee the traceability and auditability of consumed and delivered data. In other words, the Tributech DataSpace Kit is a containerized software-tool-kit that covers the whole data transfer between DataSpace Nodes (which usually represents a company's cloud or server infrastructure) and it covers the transfer of cryptographic proofs of the DataSpace Agents which enable data auditability. Based on the containerized and modular architecture, the technology can be integrated into any kind of platform , cloud or on-premise system to support the technology stack of all participating stakeholders. Check out our Integration guide for details. If you want to learn more about use-cases for your business, visit our website , visit our blog or contact our Customer Advisory Team that helps you to get started.","title":"Data exchange reinvented - Secure. Traceable. Trusted."},{"location":"introduction/product-overview.html#the-building-blocks-for-a-tributech-dataspace-ecosystem","text":"The Tributech DataSpace Kit consists of three building blocks that allow you to create your own DataSpace Ecosystem or to join an existing one. An Ecosystem can represent, for example, a value chain in which customers, suppliers and manufacturers use the DataSpace Kit in order to exchange different kinds of auditable data streams to optimize their supply chain processes. As shown below in Fig.1, these building blocks are: DataSpace Hub DataSpace Node DataSpace Agent Every ecosystem consists of one single DataSpace Hub , a DataSpace Node for each participating stakeholder integrated in his infrastructure (cloud, hybrid or on-premise) and multiple DataSpace Agents integrated into each data-source/device that requires the data auditability capabilities. The Hub holds metadata information and manages identity, required for secure auditable data sharing in the ecosystem - but is not part of the data sharing channels themselves. Actual data is shared only between nodes. The Agents feed auditable data to the nodes. Fig.1: DataSpace Kit Building Blocks & Schema_ The distributed components of the DataSpace Kit are connected via channels to exchange the different kinds of data within an ecosystem. APIs provide an interface for the data integration within the infrastructure of each connected stakeholder.","title":"The building blocks for a Tributech DataSpace Ecosystem"},{"location":"introduction/product-overview.html#next-steps","text":"Learn more about the architecture of the DataSpace Kit and how it can be integrated into your existing infrastructure Set up a demo of the product through our quick start guide","title":"Next steps"},{"location":"quickstart/create-dataset.html","text":"Create a Dataset \u00b6 Once the deployment is complete you should be able to access the DataSpace Admin App through the newly created Node under \"[deployment-name].dataspace-node.com\". You will be redirected to a login-page, enter the credentials that you specified in the setup process for the admin user. After the successful login, the Dataspace Admin App of the DataSpace Node should be visible. Follow the steps below in order to create the Dataset through usage of the DataSpace Admin App. Open the create Dataset dialog \u00b6 To create a new dataset click \"Add\" in the top right and then \"New Dataset\". Describe your Dataset \u00b6 Enter some descriptive metadata for the new dataset in the fields Name, Description and Category. You can also choose some tags for your Dataset. Add the data source and the data streams \u00b6 Add a new data-source and give the device a name of your choosing. Now create the data streams for \"Machine Temperature\", \"Machine Pressure\", \"Ambient Temperature\" and \"Ambient Humidity\". The data for these streams will later be provided by the simulated Edge Device. Successful creation of the Dataset \u00b6 Once the dataset is created, it will look somewhat like in the screenshot shown below. Each of the data streams inside of the dataset now has been automatically assigned a uniqe ID. This ID can be copied to the clipboard using the button next to each stream in the column \"ID\". These Data Stream IDs will later be relevant when setting up the simulated Edge Device. Next: Install the DSK IoT Bridge \u00b6","title":"Create a Dataset"},{"location":"quickstart/create-dataset.html#create-a-dataset","text":"Once the deployment is complete you should be able to access the DataSpace Admin App through the newly created Node under \"[deployment-name].dataspace-node.com\". You will be redirected to a login-page, enter the credentials that you specified in the setup process for the admin user. After the successful login, the Dataspace Admin App of the DataSpace Node should be visible. Follow the steps below in order to create the Dataset through usage of the DataSpace Admin App.","title":"Create a Dataset"},{"location":"quickstart/create-dataset.html#open-the-create-dataset-dialog","text":"To create a new dataset click \"Add\" in the top right and then \"New Dataset\".","title":"Open the create Dataset dialog"},{"location":"quickstart/create-dataset.html#describe-your-dataset","text":"Enter some descriptive metadata for the new dataset in the fields Name, Description and Category. You can also choose some tags for your Dataset.","title":"Describe your Dataset"},{"location":"quickstart/create-dataset.html#add-the-data-source-and-the-data-streams","text":"Add a new data-source and give the device a name of your choosing. Now create the data streams for \"Machine Temperature\", \"Machine Pressure\", \"Ambient Temperature\" and \"Ambient Humidity\". The data for these streams will later be provided by the simulated Edge Device.","title":"Add the data source and the data streams"},{"location":"quickstart/create-dataset.html#successful-creation-of-the-dataset","text":"Once the dataset is created, it will look somewhat like in the screenshot shown below. Each of the data streams inside of the dataset now has been automatically assigned a uniqe ID. This ID can be copied to the clipboard using the button next to each stream in the column \"ID\". These Data Stream IDs will later be relevant when setting up the simulated Edge Device.","title":"Successful creation of the Dataset"},{"location":"quickstart/create-dataset.html#next-install-the-dsk-iot-bridge","text":"","title":"Next: Install the DSK IoT Bridge"},{"location":"quickstart/install-bridge.html","text":"Install DSK IoT Hub Bridge \u00b6 After the node is successfully deployed we can add an instance of the Azure IoT Hub. The IoT Hub is an external service provided by Azure rather than a part of the DataSpace Kit. That is why it has to be deployed separately in order to use it. The IoT Hub provides Device-Management and Communication for our Edge Devices. While we use the Azure IoT Hub, it is also possible to integrate any other device management solution. For easier deployment of the DSK IoT Hub Bridge we prepared an Azure Resource Manager (ARM) Template. Create a custom template deployment \u00b6 Navigate to the custom template deployment \u00b6 Follow the link for the Template Deployment in the Azure Marketplace and click the \"GET IT NOW\" Button. A dialog will open where you have to agree to the terms and conditions. Navigate to the template editor \u00b6 Click \"Build your own template in the editor\" as shown below. A template editor is going to be shown. Copy and paste the prepared template \u00b6 Copy the ARM Template that can be found here into the editor and click \"Save\". Configure the deployment \u00b6 Now you will have to configure the deployment with parameters such as Resource group, Subscription etc., but also with some more specific parameters. This part will guide you through those steps. General parameters \u00b6 Pick the Subscription and Resource group where you would like the deployment to be. It is recommended to ceate a new Resource group for this or alternatively use the one that was previously created for the owner node deployment. Proceed to pick a location. As for the other settings up until including the Image Tag, the default values will most likely be fine. Node specific parameters \u00b6 This step is crucial, make sure to get the parameters right as described following. For the IoT Hub to be able to connect to your new node it requires credentials. Thus, there are a number of node specific parameters which you will have to provide. Setting Value Image Tag Current version of the DSK you are using. The default is likely fine. Data Api Url Url from the Data API Identity Server Url Token Url of the Identity Server Auth Client The Client ID Auth API Scope Scope from the Data API Auth Client Secret The Client secret You can find those parameters easiest by navigating to the Data API Swagger UI and through the DataSpace Admin Administration. Retrieve your node specific parameters from the DataSpace Admin App \u00b6 The following settings can be found in the DataSpace Admin App. Setting Value Auth Client The Client ID Auth Client Secret The Client secret In the DataSpace Admin App, navigate to the Administration as shown below. The rest of the settings can be found in the Data API Swagger UI. The IoT Hub will use the Data API to connect to the node. Navigate to the Data API Swagger UI by following the link in the DataSpace Admin App as shown in the screenshot below. A new tab will be opened showing the documentation of the Data-API. The URL will be something like https://data-api.your-node-name.dataspace-node.com/. Copy and paste the URL of your Data API as the setting \"Data Api Url\" . Next, open the Authrization dialog by clicking the \"Authorize\" Button in the Swagger UI. You can find the setting \"Identity Server Url\" there through the \"Token URL\" parameter in the dialog. Check the screenshot below for further info. Note that you must only take the part of the Token URL without the /connect/token part. All that's missing now is the \"Auth API Scope\" setting. You can find this through the \"Scopes\" section at the bottom of the Authorization dialog. Note that it is only the upper value shown there, as highlighted through the red box in the screenshot below. This value will most likely be data-api-endpoint . Finalize the deployment \u00b6 After inserting all the parameters and settings, click \"Purchase\". The deployment will run for a couple of minutes. After the deployment is finished there should be a new instance of the Azure IoT Hub in the selected Resource group. This IoT Hub will be used in the following steps to deploy a simulated Edge Device. Next: Setup an IoT device \u00b6","title":"Install DSK IoT Bridge"},{"location":"quickstart/install-bridge.html#install-dsk-iot-hub-bridge","text":"After the node is successfully deployed we can add an instance of the Azure IoT Hub. The IoT Hub is an external service provided by Azure rather than a part of the DataSpace Kit. That is why it has to be deployed separately in order to use it. The IoT Hub provides Device-Management and Communication for our Edge Devices. While we use the Azure IoT Hub, it is also possible to integrate any other device management solution. For easier deployment of the DSK IoT Hub Bridge we prepared an Azure Resource Manager (ARM) Template.","title":"Install DSK IoT Hub Bridge"},{"location":"quickstart/install-bridge.html#create-a-custom-template-deployment","text":"","title":"Create a custom template deployment"},{"location":"quickstart/install-bridge.html#navigate-to-the-custom-template-deployment","text":"Follow the link for the Template Deployment in the Azure Marketplace and click the \"GET IT NOW\" Button. A dialog will open where you have to agree to the terms and conditions.","title":"Navigate to the custom template deployment"},{"location":"quickstart/install-bridge.html#navigate-to-the-template-editor","text":"Click \"Build your own template in the editor\" as shown below. A template editor is going to be shown.","title":"Navigate to the template editor"},{"location":"quickstart/install-bridge.html#copy-and-paste-the-prepared-template","text":"Copy the ARM Template that can be found here into the editor and click \"Save\".","title":"Copy and paste the prepared template"},{"location":"quickstart/install-bridge.html#configure-the-deployment","text":"Now you will have to configure the deployment with parameters such as Resource group, Subscription etc., but also with some more specific parameters. This part will guide you through those steps.","title":"Configure the deployment"},{"location":"quickstart/install-bridge.html#general-parameters","text":"Pick the Subscription and Resource group where you would like the deployment to be. It is recommended to ceate a new Resource group for this or alternatively use the one that was previously created for the owner node deployment. Proceed to pick a location. As for the other settings up until including the Image Tag, the default values will most likely be fine.","title":"General parameters"},{"location":"quickstart/install-bridge.html#node-specific-parameters","text":"This step is crucial, make sure to get the parameters right as described following. For the IoT Hub to be able to connect to your new node it requires credentials. Thus, there are a number of node specific parameters which you will have to provide. Setting Value Image Tag Current version of the DSK you are using. The default is likely fine. Data Api Url Url from the Data API Identity Server Url Token Url of the Identity Server Auth Client The Client ID Auth API Scope Scope from the Data API Auth Client Secret The Client secret You can find those parameters easiest by navigating to the Data API Swagger UI and through the DataSpace Admin Administration.","title":"Node specific parameters"},{"location":"quickstart/install-bridge.html#finalize-the-deployment","text":"After inserting all the parameters and settings, click \"Purchase\". The deployment will run for a couple of minutes. After the deployment is finished there should be a new instance of the Azure IoT Hub in the selected Resource group. This IoT Hub will be used in the following steps to deploy a simulated Edge Device.","title":"Finalize the deployment"},{"location":"quickstart/install-bridge.html#next-setup-an-iot-device","text":"","title":"Next: Setup an IoT device"},{"location":"quickstart/install-consumer-node.html","text":"Install Customer Node \u00b6 For the setup of the consumer node, the same steps as in Install Owner Node are required. It is possible to set the nodes up in two different Azure subscriptions if you want to create a more realistic testcase with a partner company. Make sure to select a differing, unique node name. Next: Subscribe to Dataset \u00b6","title":"Install Consumer Node"},{"location":"quickstart/install-consumer-node.html#install-customer-node","text":"For the setup of the consumer node, the same steps as in Install Owner Node are required. It is possible to set the nodes up in two different Azure subscriptions if you want to create a more realistic testcase with a partner company. Make sure to select a differing, unique node name.","title":"Install Customer Node"},{"location":"quickstart/install-consumer-node.html#next-subscribe-to-dataset","text":"","title":"Next: Subscribe to Dataset"},{"location":"quickstart/install-owner-node.html","text":"Installing the data owner node \u00b6 A demo of a Tributech DataSpace Node can be deployed with only a few clicks through the Azure Marketplace , as explained below. Go to the Azure Marketplace \u00b6 In the maketplace simply select the option \"Trial Hub\" and click \"Create\". Fill in the basic information \u00b6 In the first step of the form fill in your desired Resource Group and Region and click next. It is a good idea to create a new Resource Group for this demo. Enter user information \u00b6 Enter personal information for the user-account which will be created automatically on the node. Then select a name for your node and enter it in the field \"Deployment Name\". Note that this name has to be uniqe. This name will be part of the URL of your node. After the deployment is finished, your node will be reachable under \"[deployment-name].dataspace-node.com\". Take note of this URL for later steps. Select VM Size \u00b6 In the resource settings it is possible to select the VM Size. The default will work fine for most cases. Check out the node requirements for more information. Review and Create \u00b6 Finally, review the entered information and click \"Create\". Keep in mind that the deployment process takes some time to complete. Why not go and get a coffee now. Next: Create a Dataset \u00b6","title":"Install Owner Node"},{"location":"quickstart/install-owner-node.html#installing-the-data-owner-node","text":"A demo of a Tributech DataSpace Node can be deployed with only a few clicks through the Azure Marketplace , as explained below.","title":"Installing the data owner node"},{"location":"quickstart/install-owner-node.html#go-to-the-azure-marketplace","text":"In the maketplace simply select the option \"Trial Hub\" and click \"Create\".","title":"Go to the Azure Marketplace"},{"location":"quickstart/install-owner-node.html#fill-in-the-basic-information","text":"In the first step of the form fill in your desired Resource Group and Region and click next. It is a good idea to create a new Resource Group for this demo.","title":"Fill in the basic information"},{"location":"quickstart/install-owner-node.html#enter-user-information","text":"Enter personal information for the user-account which will be created automatically on the node. Then select a name for your node and enter it in the field \"Deployment Name\". Note that this name has to be uniqe. This name will be part of the URL of your node. After the deployment is finished, your node will be reachable under \"[deployment-name].dataspace-node.com\". Take note of this URL for later steps.","title":"Enter user information"},{"location":"quickstart/install-owner-node.html#select-vm-size","text":"In the resource settings it is possible to select the VM Size. The default will work fine for most cases. Check out the node requirements for more information.","title":"Select VM Size"},{"location":"quickstart/install-owner-node.html#review-and-create","text":"Finally, review the entered information and click \"Create\". Keep in mind that the deployment process takes some time to complete. Why not go and get a coffee now.","title":"Review and Create"},{"location":"quickstart/install-owner-node.html#next-create-a-dataset","text":"","title":"Next: Create a Dataset"},{"location":"quickstart/overview.html","text":"Quickstart Guide \u00b6 This guide describes how to quickly get up and running with the Tributech DataSpace Kit (DSK). The easiest way to get started with only a few clicks is in the Azure Cloud. While the Azure Cloud is the quickest and easiest way to get started, other options for deployment, including on premise systems, are also available. Check out our Setup Guide for detail information.. Demonstrated Use-Case \u00b6 This quickstart guide demonstrates how the DataSpace Kit can be used to share data between two parties. This could for example enable you to share sensor data from production systems securely with a provider of machine learning services to enable predictive maintenance. The guide shows a Hello-World example that covers all the important features of the DataSpace Kit. It includes a simulated edge device which continuously generates several streams of simulated sensor-data. This dataset is then shared with a Data-Consumer node. If the demo environment works for you, we can upgrade it into a full production ready system. Feel free to contact us . In the image below you can see the data flow that will be established through this guide: From data source to data owner, which shares selected streams with the data consumer. The data consumer can then verify data integrity and data authenticity through a specific data auditing service. Features \u00b6 Data never leaves your system unless explicitly shared Data is collected and signed on the Edge to make it auditable The Data Consumer can request access to a shared dataset Once access is granted the generated data is continuously shared with the consumer (but could be revoked at any time) The consumer of the auditable data stream can now access the data for example to use it for machine learning The consumer can cryptographically verify the data authenticity and data integrity This Guide Covers \u00b6 How to setup the Data-Owner-Node in a few clicks How to create the first dataset How to link the Azure IoT Hub to the Data-Owner-Node How to deploy a simulated edge device as a data-source using the Azure IoT Hub How to share the dataset containing the values from the simulated sensor How to setup another node as data-consumer to simulate a partner company How to subscribe to a shared dataset of the data-owner Next: Install the owner node \u00b6","title":"Overview"},{"location":"quickstart/overview.html#quickstart-guide","text":"This guide describes how to quickly get up and running with the Tributech DataSpace Kit (DSK). The easiest way to get started with only a few clicks is in the Azure Cloud. While the Azure Cloud is the quickest and easiest way to get started, other options for deployment, including on premise systems, are also available. Check out our Setup Guide for detail information..","title":"Quickstart Guide"},{"location":"quickstart/overview.html#demonstrated-use-case","text":"This quickstart guide demonstrates how the DataSpace Kit can be used to share data between two parties. This could for example enable you to share sensor data from production systems securely with a provider of machine learning services to enable predictive maintenance. The guide shows a Hello-World example that covers all the important features of the DataSpace Kit. It includes a simulated edge device which continuously generates several streams of simulated sensor-data. This dataset is then shared with a Data-Consumer node. If the demo environment works for you, we can upgrade it into a full production ready system. Feel free to contact us . In the image below you can see the data flow that will be established through this guide: From data source to data owner, which shares selected streams with the data consumer. The data consumer can then verify data integrity and data authenticity through a specific data auditing service.","title":"Demonstrated Use-Case"},{"location":"quickstart/overview.html#features","text":"Data never leaves your system unless explicitly shared Data is collected and signed on the Edge to make it auditable The Data Consumer can request access to a shared dataset Once access is granted the generated data is continuously shared with the consumer (but could be revoked at any time) The consumer of the auditable data stream can now access the data for example to use it for machine learning The consumer can cryptographically verify the data authenticity and data integrity","title":"Features"},{"location":"quickstart/overview.html#this-guide-covers","text":"How to setup the Data-Owner-Node in a few clicks How to create the first dataset How to link the Azure IoT Hub to the Data-Owner-Node How to deploy a simulated edge device as a data-source using the Azure IoT Hub How to share the dataset containing the values from the simulated sensor How to setup another node as data-consumer to simulate a partner company How to subscribe to a shared dataset of the data-owner","title":"This Guide Covers"},{"location":"quickstart/overview.html#next-install-the-owner-node","text":"","title":"Next: Install the owner node"},{"location":"quickstart/publish-dataset.html","text":"Publish the Dataset \u00b6 Now that the values are streaming in, we want to simulate the consumer part of the auditable data sharing. The Dataset has already been created in a previosu step and can now be published, so that a data-consumer can see and consume the data. Add Publication \u00b6 In the DataSpace Admin App, navigate to \"My Datasets\" in the left side menu. Then select one of your Datasets (probably the previously created in step Create a Dataset ) and click on \"Add Publication\" for the selected Dataset Select Audience \u00b6 As a first step you will have to set the Audience. Publishing a Dataset means that you make the descriptive metadata visible for other members of your DataSpace Ecosystem, so they can request access to your Datasets and subscribe to them if you grant access. If you want to publish a Dataset you have two different options. Selecting \"Public\" means that metadata is visible for all members of the Ecosystem. The data is not automatically shared with all members. Instead other members will get the option to request access to your dataset. Selecting \"Invite\" means that the data will only be visible to a selected group of members, which you invite explicitly. For the purpose of this guide select \"Public\". Select sources and streams \u00b6 Select the sources and streams you want to share. As you see in the example you can selectively choose which streams you want to publish. If you choose a data source you can select which streams of the data source you want to publish. For this example, select all the streams for the data source. Select a timeframe \u00b6 Next, select a timeframe for your Publication. This timeframe sets the period that possible data-consumers can request to subscribe for. The real consumable timeframe depends on the availability of each stream. The available date for each stream is defined by the metadata field \"Available from\" that is set initially when the stream of the Dataset is created. Select a contract \u00b6 If you also want to include some governance, you have the possibility to upload a contract that contains the terms of use that must be agreed upon as part of the request process. Select the checkbox and upload a PDF file containing your terms of use. Publish your Dataset \u00b6 Before completing the publishing process, please review your selection. After you publish a Dataset, it will be visible for other members (depending on the Audience selection) in your DataSpace Ecosystem. After submission, other members can send a Request if they want to consume your data. Next: Install Consumer Node \u00b6","title":"Publish Dataset"},{"location":"quickstart/publish-dataset.html#publish-the-dataset","text":"Now that the values are streaming in, we want to simulate the consumer part of the auditable data sharing. The Dataset has already been created in a previosu step and can now be published, so that a data-consumer can see and consume the data.","title":"Publish the Dataset"},{"location":"quickstart/publish-dataset.html#add-publication","text":"In the DataSpace Admin App, navigate to \"My Datasets\" in the left side menu. Then select one of your Datasets (probably the previously created in step Create a Dataset ) and click on \"Add Publication\" for the selected Dataset","title":"Add Publication"},{"location":"quickstart/publish-dataset.html#select-audience","text":"As a first step you will have to set the Audience. Publishing a Dataset means that you make the descriptive metadata visible for other members of your DataSpace Ecosystem, so they can request access to your Datasets and subscribe to them if you grant access. If you want to publish a Dataset you have two different options. Selecting \"Public\" means that metadata is visible for all members of the Ecosystem. The data is not automatically shared with all members. Instead other members will get the option to request access to your dataset. Selecting \"Invite\" means that the data will only be visible to a selected group of members, which you invite explicitly. For the purpose of this guide select \"Public\".","title":"Select Audience"},{"location":"quickstart/publish-dataset.html#select-sources-and-streams","text":"Select the sources and streams you want to share. As you see in the example you can selectively choose which streams you want to publish. If you choose a data source you can select which streams of the data source you want to publish. For this example, select all the streams for the data source.","title":"Select sources and streams"},{"location":"quickstart/publish-dataset.html#select-a-timeframe","text":"Next, select a timeframe for your Publication. This timeframe sets the period that possible data-consumers can request to subscribe for. The real consumable timeframe depends on the availability of each stream. The available date for each stream is defined by the metadata field \"Available from\" that is set initially when the stream of the Dataset is created.","title":"Select a timeframe"},{"location":"quickstart/publish-dataset.html#select-a-contract","text":"If you also want to include some governance, you have the possibility to upload a contract that contains the terms of use that must be agreed upon as part of the request process. Select the checkbox and upload a PDF file containing your terms of use.","title":"Select a contract"},{"location":"quickstart/publish-dataset.html#publish-your-dataset","text":"Before completing the publishing process, please review your selection. After you publish a Dataset, it will be visible for other members (depending on the Audience selection) in your DataSpace Ecosystem. After submission, other members can send a Request if they want to consume your data.","title":"Publish your Dataset"},{"location":"quickstart/publish-dataset.html#next-install-consumer-node","text":"","title":"Next: Install Consumer Node"},{"location":"quickstart/setup-device.html","text":"Setup IoT Device \u00b6 Now that we have the IoT hub set up, we can create the Edge Device for the IoT Hub. We will then deploy a VM as a simulated IoT Edge Runtime and deploy the Edge Runtime Modules which provide the simulation data. Create Edge Device in IoT Hub \u00b6 We start with creating the Edge Device in the IoT Hub. In the just deployed resources from the previous step of installing the DSK IoT Bridge you should find the IoT Hub resource. Open this resource in the Azure Portal. As shown in the screenshow below, next, navigate through the side menu to \"IoT Edge\". In the overview click on \"Add an IoT Edge device\". Give your device a name and click \"Save\". Your device will be created. When the device is created, open the device details by clicking it. Leave this tab open, we will need the primary connetion string in just a second. Create simulated IoT Edge Runtime VM \u00b6 Now we will deploy a simulated IoT Edge Runtime and connect it to our just created device. In a new tab, open the \"Azure IoT Edge on Ubuntu\" Azure Marketplace listing. The simulated Edge Runtime Device is simply an Ubuntu VM with the Azure IoT Edge Runtime installed. Choose a good name for the VM. The Size \"Standard_B1s\" should be fine for the purpose of this guide. If you are planning on processing large amounts of data consider using a more powerful instance. Configure SSH so that you can log into the VM after it has been created, e.g. through setting a password. Create the VM by following the \"Review + create\" steps. After the VM has been created log into the VM via SSH in a terminal of your choosing and run the following commands. If you created the SSH access via password, then you can connect through the command ssh username@[your-vm-ip-address] Note that you can find the ip-address of your vm by navigating to the created VM in the Azure portal. You will be prompted to enter the password that you have configured. Finally, execute the following commands in the terminal connected via SSH to the VM. Use the primary connection string from the previously created device, which you should still have available in the device details tab. Replace the \"[IoT-Hub-Primary-Connection-String]\" with your primary connection string. sudo /etc/iotedge/configedge.sh \"[IoT-Hub-Primary-Connection-String]\" sudo apt-get update sudo apt-get upgrade -y sudo reboot Deploy Edge Runtime Modules \u00b6 You will need two Azure Edge Runtime modules. A simulated source of sensor values and the DSK Agent Edge. The job of the data-source is to create simulated values. The Agent is responsible for signing the data and sending the data as well as the signatures (proofs) to the DSK Node. Add Simulated Temperature Sensor Module \u00b6 The Simulated Temperature Sensor is provided by Azure IoT and can be found in the Marketplace . Create a new Simulated Temperature Sensor by clicking \"Create\" in the Azure Marketplace listing: Select your IoT Hub and Edge Device as Target devices for the module and click \"Create\": Configure the Simulated Temperature Sensor Module \u00b6 After the Simulated Temperature Sensor Module has been created, click on the new module to open its settings: In the settings, in the tab \"Environment Variables\", set up the variable \"MessageCount\" and give it a value of -1. This will allow for an infinite number of sensor readings. After that click on \"Update\": Finalize the setup of the Simulated Temperature Sensor Module by following the \"Review + create\" steps: Add DSK Agent Edge Module \u00b6 The DSK Agent Edge can also be found in the Marketplace . Proceed the same way as previously for the Simulated Temperature Sensor Module: Click \"Create\" in the Azure Marketplace listing and select your IoT Hub and the Edge device and click \"Create\", shown in the screenshots below. Configure the DSK Agent Edge Module \u00b6 Some environment variables must be configured for the agent. To be able to create signatures, the Agent needs to be connected to the Trust-API of the DSK Node which requires API-Credentials. The credentials for the Trust-API can be obtained the same way as the credentials for the Data-API have been obtained when installing the DSK IoT Bridge . Open the module by clicking it in the list of IoT Edge Modules and navigate to the tab \"Environment Variables\". There you will have a number of variables, which are explained below the screenshot. Variable name Value Sample EdgeDeviceOptions__AgentID Choose a unique string -> you can easily generate a GUID online 840c1996-2761-4d90-b390-3ecbd8879015 ProofSinkOptions__TrustAPIBaseUrl Trust API Url. Replace <your-node-name> of the sample with the name of your node. https://trust-api.<your-node-name>.dataspace-node.com ProofSinkOptions__AuthUrl Auth Url from Trust API Swagger UI authorization dialog. You can simply replace <your-hub-name> of the sample with the name of your hub. The default hub name azuretrial should work for most cases. https://id.<your-hub-name>.dataspace-hub.com/connect/token ProofSinkOptions__AuthScope Auth Scope from Trust API Swagger UI authorization dialog. The sample value should work for most cases. data-api-endpoint trust-api-endpoint ProofSinkOptions__ClientID Client ID from the DataSpace Admin 9ee39d88-2961-4296-a94e-a1841e454076 ProofSinkOptions__ClientSecret Client Secret from the DataSpace Admin ed2a4425-81ad-4a00-8300-affa01d79b46 EdgeDeviceOptions__DataStreamID__[DataStreamID] Replace [DataStreamID] from the variable name with the DataStream ID generated in the step Create a Dataset SimulatedTemperatureSensorValueSource__0__Machine_Temperature As for the last variable, the EdgeDeviceOptions__DataStreamID__[DataStreamID] , you will have to set up 4 variables for it: Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Machine_Temperature Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Machine_Pressure Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Ambient_Temperature Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Ambient_Humidity Note that you can easily copy the DataStreamID by clicking the copy button in the column ID as explained in the step Create a Dataset . Finally, double check your variables and click \"Update\". Then follow the \"Review + Create\" process. Configure Routes \u00b6 All that's left now is the configuration of the routes, so that the simulated data actually ends up where it should. Navigate to the routes configuration \u00b6 First, open the IoT Hub in the Azure portal and select the device: Then, click on \"Set Modules\" and navigate to the routes by clicking \"Next: Routes\": Set the routes \u00b6 Lastly, set the following two routes: Name Route SimulatedSensorToDSKAgent FROM /messages/modules/SimulatedTemperatureSensor/outputs/temperatureOutput INTO BrokeredEndpoint(\"/modules/TributechDataSpaceAgent/inputs/AzureSimulatedSensorValue\") DSKEdgeAgentValuesToCloud FROM /messages/modules/TributechDataSpaceAgent/outputs/ValueSink INTO $upstream Two routes are required, one for sending the simulated sensor data to the DSK Agent and one for transmission of the DSK Agent Edge to the IoT Hub. Finish the process by clicking \"Review + create\" and following the process. Verify that messages arrive at IoT Hub \u00b6 You can verify that everything works as expected by checking the Overview dashboard of your IoT Hub in the Azure Portal: You can also already create your first dashboard and have the simulated data presented visually. Just follow the guide for consuming data via Dashboards . Note that for this case you would be in the position of data owner. Next: Publish Dataset \u00b6","title":"Setup IoT Device"},{"location":"quickstart/setup-device.html#setup-iot-device","text":"Now that we have the IoT hub set up, we can create the Edge Device for the IoT Hub. We will then deploy a VM as a simulated IoT Edge Runtime and deploy the Edge Runtime Modules which provide the simulation data.","title":"Setup IoT Device"},{"location":"quickstart/setup-device.html#create-edge-device-in-iot-hub","text":"We start with creating the Edge Device in the IoT Hub. In the just deployed resources from the previous step of installing the DSK IoT Bridge you should find the IoT Hub resource. Open this resource in the Azure Portal. As shown in the screenshow below, next, navigate through the side menu to \"IoT Edge\". In the overview click on \"Add an IoT Edge device\". Give your device a name and click \"Save\". Your device will be created. When the device is created, open the device details by clicking it. Leave this tab open, we will need the primary connetion string in just a second.","title":"Create Edge Device in IoT Hub"},{"location":"quickstart/setup-device.html#create-simulated-iot-edge-runtime-vm","text":"Now we will deploy a simulated IoT Edge Runtime and connect it to our just created device. In a new tab, open the \"Azure IoT Edge on Ubuntu\" Azure Marketplace listing. The simulated Edge Runtime Device is simply an Ubuntu VM with the Azure IoT Edge Runtime installed. Choose a good name for the VM. The Size \"Standard_B1s\" should be fine for the purpose of this guide. If you are planning on processing large amounts of data consider using a more powerful instance. Configure SSH so that you can log into the VM after it has been created, e.g. through setting a password. Create the VM by following the \"Review + create\" steps. After the VM has been created log into the VM via SSH in a terminal of your choosing and run the following commands. If you created the SSH access via password, then you can connect through the command ssh username@[your-vm-ip-address] Note that you can find the ip-address of your vm by navigating to the created VM in the Azure portal. You will be prompted to enter the password that you have configured. Finally, execute the following commands in the terminal connected via SSH to the VM. Use the primary connection string from the previously created device, which you should still have available in the device details tab. Replace the \"[IoT-Hub-Primary-Connection-String]\" with your primary connection string. sudo /etc/iotedge/configedge.sh \"[IoT-Hub-Primary-Connection-String]\" sudo apt-get update sudo apt-get upgrade -y sudo reboot","title":"Create simulated IoT Edge Runtime VM"},{"location":"quickstart/setup-device.html#deploy-edge-runtime-modules","text":"You will need two Azure Edge Runtime modules. A simulated source of sensor values and the DSK Agent Edge. The job of the data-source is to create simulated values. The Agent is responsible for signing the data and sending the data as well as the signatures (proofs) to the DSK Node.","title":"Deploy Edge Runtime Modules"},{"location":"quickstart/setup-device.html#add-simulated-temperature-sensor-module","text":"The Simulated Temperature Sensor is provided by Azure IoT and can be found in the Marketplace . Create a new Simulated Temperature Sensor by clicking \"Create\" in the Azure Marketplace listing: Select your IoT Hub and Edge Device as Target devices for the module and click \"Create\":","title":"Add Simulated Temperature Sensor Module"},{"location":"quickstart/setup-device.html#configure-the-simulated-temperature-sensor-module","text":"After the Simulated Temperature Sensor Module has been created, click on the new module to open its settings: In the settings, in the tab \"Environment Variables\", set up the variable \"MessageCount\" and give it a value of -1. This will allow for an infinite number of sensor readings. After that click on \"Update\": Finalize the setup of the Simulated Temperature Sensor Module by following the \"Review + create\" steps:","title":"Configure the Simulated Temperature Sensor Module"},{"location":"quickstart/setup-device.html#add-dsk-agent-edge-module","text":"The DSK Agent Edge can also be found in the Marketplace . Proceed the same way as previously for the Simulated Temperature Sensor Module: Click \"Create\" in the Azure Marketplace listing and select your IoT Hub and the Edge device and click \"Create\", shown in the screenshots below.","title":"Add DSK Agent Edge Module"},{"location":"quickstart/setup-device.html#configure-the-dsk-agent-edge-module","text":"Some environment variables must be configured for the agent. To be able to create signatures, the Agent needs to be connected to the Trust-API of the DSK Node which requires API-Credentials. The credentials for the Trust-API can be obtained the same way as the credentials for the Data-API have been obtained when installing the DSK IoT Bridge . Open the module by clicking it in the list of IoT Edge Modules and navigate to the tab \"Environment Variables\". There you will have a number of variables, which are explained below the screenshot. Variable name Value Sample EdgeDeviceOptions__AgentID Choose a unique string -> you can easily generate a GUID online 840c1996-2761-4d90-b390-3ecbd8879015 ProofSinkOptions__TrustAPIBaseUrl Trust API Url. Replace <your-node-name> of the sample with the name of your node. https://trust-api.<your-node-name>.dataspace-node.com ProofSinkOptions__AuthUrl Auth Url from Trust API Swagger UI authorization dialog. You can simply replace <your-hub-name> of the sample with the name of your hub. The default hub name azuretrial should work for most cases. https://id.<your-hub-name>.dataspace-hub.com/connect/token ProofSinkOptions__AuthScope Auth Scope from Trust API Swagger UI authorization dialog. The sample value should work for most cases. data-api-endpoint trust-api-endpoint ProofSinkOptions__ClientID Client ID from the DataSpace Admin 9ee39d88-2961-4296-a94e-a1841e454076 ProofSinkOptions__ClientSecret Client Secret from the DataSpace Admin ed2a4425-81ad-4a00-8300-affa01d79b46 EdgeDeviceOptions__DataStreamID__[DataStreamID] Replace [DataStreamID] from the variable name with the DataStream ID generated in the step Create a Dataset SimulatedTemperatureSensorValueSource__0__Machine_Temperature As for the last variable, the EdgeDeviceOptions__DataStreamID__[DataStreamID] , you will have to set up 4 variables for it: Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Machine_Temperature Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Machine_Pressure Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Ambient_Temperature Name: EdgeDeviceOptions__DataStreamID__[DataStreamID] Value: SimulatedTemperatureSensorValueSource__0__Ambient_Humidity Note that you can easily copy the DataStreamID by clicking the copy button in the column ID as explained in the step Create a Dataset . Finally, double check your variables and click \"Update\". Then follow the \"Review + Create\" process.","title":"Configure the DSK Agent Edge Module"},{"location":"quickstart/setup-device.html#configure-routes","text":"All that's left now is the configuration of the routes, so that the simulated data actually ends up where it should.","title":"Configure Routes"},{"location":"quickstart/setup-device.html#verify-that-messages-arrive-at-iot-hub","text":"You can verify that everything works as expected by checking the Overview dashboard of your IoT Hub in the Azure Portal: You can also already create your first dashboard and have the simulated data presented visually. Just follow the guide for consuming data via Dashboards . Note that for this case you would be in the position of data owner.","title":"Verify that messages arrive at IoT Hub"},{"location":"quickstart/setup-device.html#next-publish-dataset","text":"","title":"Next: Publish Dataset"},{"location":"quickstart/subscribe-dataset.html","text":"Subscribe to a Dataset \u00b6 After the setup of the consumer node is complete, the consumer node can subscribe to the dataset that was created before with the owner node. Execute the following steps by accessing the DataSpace Admin App of the consumer node, e.g. in a new private browser window. Create the Request for a Subscription \u00b6 You can see all available data streams by navigating to \"DataSpace Hub\" in the left side menu of the DataSpace Admin App. In this data preview you can browse through the list of available data sources and streams and view their metadata. To create a Request to Subscribe to a data stream, click on the \"Subscribe to Data\" button. This will open up a dialog similar to the one shown in the screenshot below. Select the data streams for which you would like to send the Subscription request for and click \"Next Step\". Note: You can selectively choose data streams out of one dataset. You don't have to subscribe to all data streams of a dataset. Next, you can select the timeframe for which you want to subscribe and consume the selected data streams. The dates for when the stream is generally available (\"Available from\" and \"Available to\") of each data stream are also shown. In the next step, you might have to agree to terms and conditions of use, if the data owner has defined such a document. Finally, once you have selected the data sources and streams of your choice, set the timeframe and agreed to the terms and conditions of use, you can send a Request to the owner of the selected dataset by clicking \"Send Request to Subscribe\". Note: You also have the possibility to add a comment for the data owner to your Request. Your Dataset Subscription \u00b6 After the Request has been sent, a new entry appears in the \"Subscriptions\" sections of your DataSpace Node (again reachable through the left side menu). The initial state of this Subscription is \"pending\" until the owner of the dataset grants or denies your Request. You can switch back to the DataSpace Admin App of the Data Owner Node and grant the request. Once your Request is granted by the owner, the state changes to granted and the synchronisation process starts - depending on the selected timeframe and available data on the owner side. Note: Of course the data owner also has the possibility to revoke access to the shared data stream at all times. Next: Consuming data via Dashboards \u00b6","title":"Subscribe to Dataset"},{"location":"quickstart/subscribe-dataset.html#subscribe-to-a-dataset","text":"After the setup of the consumer node is complete, the consumer node can subscribe to the dataset that was created before with the owner node. Execute the following steps by accessing the DataSpace Admin App of the consumer node, e.g. in a new private browser window.","title":"Subscribe to a Dataset"},{"location":"quickstart/subscribe-dataset.html#create-the-request-for-a-subscription","text":"You can see all available data streams by navigating to \"DataSpace Hub\" in the left side menu of the DataSpace Admin App. In this data preview you can browse through the list of available data sources and streams and view their metadata. To create a Request to Subscribe to a data stream, click on the \"Subscribe to Data\" button. This will open up a dialog similar to the one shown in the screenshot below. Select the data streams for which you would like to send the Subscription request for and click \"Next Step\". Note: You can selectively choose data streams out of one dataset. You don't have to subscribe to all data streams of a dataset. Next, you can select the timeframe for which you want to subscribe and consume the selected data streams. The dates for when the stream is generally available (\"Available from\" and \"Available to\") of each data stream are also shown. In the next step, you might have to agree to terms and conditions of use, if the data owner has defined such a document. Finally, once you have selected the data sources and streams of your choice, set the timeframe and agreed to the terms and conditions of use, you can send a Request to the owner of the selected dataset by clicking \"Send Request to Subscribe\". Note: You also have the possibility to add a comment for the data owner to your Request.","title":"Create the Request for a Subscription"},{"location":"quickstart/subscribe-dataset.html#your-dataset-subscription","text":"After the Request has been sent, a new entry appears in the \"Subscriptions\" sections of your DataSpace Node (again reachable through the left side menu). The initial state of this Subscription is \"pending\" until the owner of the dataset grants or denies your Request. You can switch back to the DataSpace Admin App of the Data Owner Node and grant the request. Once your Request is granted by the owner, the state changes to granted and the synchronisation process starts - depending on the selected timeframe and available data on the owner side. Note: Of course the data owner also has the possibility to revoke access to the shared data stream at all times.","title":"Your Dataset Subscription"},{"location":"quickstart/subscribe-dataset.html#next-consuming-data-via-dashboards","text":"","title":"Next: Consuming data via Dashboards"},{"location":"quickstart/data-consuming/data-via-apis.html","text":"Consume data via APIs \u00b6 In this guide you will learn how you can consume your own Datasets and Subscriptions via the built in APIs of your DataSpace Node. Prerequisites \u00b6 The prerequisits for consuming data from a Subscription via the integrated APIs are, that a Dataset must be available at your node. If you followed the Quick Start guide, then those prerequisites should be fulfilled. This could be in one of two ways: You are Data Owner : You own some Datasets and have your own data on your node (Owner DB) You are Data Consumer : You have been granted a Subscription to a Dataset and the data is already synchronised to your node (Consumer DB) General info \u00b6 Navigate to the Data API / Trust API of your node and Authorize yourself by following our Swagger UI Authorization Guide . You can retrieve the valueMetadataId which is a unique id for a data stream by copying it in the DataSpace Admin App as described here . Retrieving values through the Data API \u00b6 You can retrieve values for one of your data streams through the /values/{valueMetadataId} endpoint. Click \"Try it out\", paste the valueMetadataId, scroll down and click \"Execute\". Other endpoints \u00b6 You can access all other endpoints the same way. Note that for a lot of endpoints you will also need a timestamp. A ValueMetadataId and a Timestamp together uniquely identify a certain value. Next steps \u00b6 For further details about API integration, read our DSK Node Integration Guide .","title":"Via APIs"},{"location":"quickstart/data-consuming/data-via-apis.html#consume-data-via-apis","text":"In this guide you will learn how you can consume your own Datasets and Subscriptions via the built in APIs of your DataSpace Node.","title":"Consume data via APIs"},{"location":"quickstart/data-consuming/data-via-apis.html#prerequisites","text":"The prerequisits for consuming data from a Subscription via the integrated APIs are, that a Dataset must be available at your node. If you followed the Quick Start guide, then those prerequisites should be fulfilled. This could be in one of two ways: You are Data Owner : You own some Datasets and have your own data on your node (Owner DB) You are Data Consumer : You have been granted a Subscription to a Dataset and the data is already synchronised to your node (Consumer DB)","title":"Prerequisites"},{"location":"quickstart/data-consuming/data-via-apis.html#general-info","text":"Navigate to the Data API / Trust API of your node and Authorize yourself by following our Swagger UI Authorization Guide . You can retrieve the valueMetadataId which is a unique id for a data stream by copying it in the DataSpace Admin App as described here .","title":"General info"},{"location":"quickstart/data-consuming/data-via-apis.html#retrieving-values-through-the-data-api","text":"You can retrieve values for one of your data streams through the /values/{valueMetadataId} endpoint. Click \"Try it out\", paste the valueMetadataId, scroll down and click \"Execute\".","title":"Retrieving values through the Data API"},{"location":"quickstart/data-consuming/data-via-apis.html#other-endpoints","text":"You can access all other endpoints the same way. Note that for a lot of endpoints you will also need a timestamp. A ValueMetadataId and a Timestamp together uniquely identify a certain value.","title":"Other endpoints"},{"location":"quickstart/data-consuming/data-via-apis.html#next-steps","text":"For further details about API integration, read our DSK Node Integration Guide .","title":"Next steps"},{"location":"quickstart/data-consuming/data-via-dashboards.html","text":"Consume data via dashbaords \u00b6 In this guide you will learn how you can consume your own Datasets and your Subscriptions via the built in dashboards of your DataSpace Node. This guide is layed out for the data streams in the Quick start guide, although creating dashboards for other data streams should follow the same principle, just end up with a slightly different configuration, depending on the needs. Prerequisites \u00b6 The prerequisits for consuming data from a Subscription via the integrated Dashboards are, that a Dataset must be available at your node. If you followed the Quick Start guide, then those prerequisites should be fulfilled. This could be in one of two ways: You are Data Owner : You own some Datasets and have your own data on your node (Owner DB) You are Data Consumer : You have been granted a Subscription to a Dataset and the data is already synchronised to your node (Consumer DB) Node Dashboard Basics \u00b6 You can easily navigate to your node dashboard by clicking the \"Grafana\" menu entry in the side menu of your DataSpace Admin App. For authentication, use the same username and password that you use for authentication when accessing the DataSpace Admin App. The dashboarding system is built on the open source technology Grafana . It comes with pre-configured data sources for the in your DataSpace Node integrated Postgre SQL databases. The structure of dashboards is based on the same logic as in the interface of your DataSpace Node: Dashboards: My Datasets / Owner DB - contains owned Datasets that are connected to your DataSpace Node. Dashboards: Subscriptions / Consumer DB - contains subscriptions to Datasets from other members of your Ecosystem. Create a Dashboard \u00b6 To visualize your data via the dashboarding system the only thing you have to do is to duplicate the pre-configured template, paste a code snippet and paste the UUID (ValueMetadataId) of your slected data stream. 1. Add a new Dashboard - Click the \"+\" button and click on the \"Dashboard\" menu item to create a new Dashboard. Then click on the settings symbol to get to the Dashboard settings of the just created Dashboard, as shown below: 2. Copy and paste the JSON Model template - First, prepare the JSON template and adjust to your unique stream ids. If you are Owner, then copy the json of the Owner DB JSON template If you are Consumer, then copy the json of the Consumer DB JSON template Copy the contents to a editor of your choice. In the editor, paste the template JSON and search for \"ReplaceWith\". There should be 4 times where you must replace the <ReplaceWith...> strings with the IDs of your data streams, which you can find as described here . Finally, copy this adjusted template JSON, paste into the settings tab \"JSON Model\" of your new dashboard and click \"Save Changes\". Optionally, you can now rename the rows / panels to your liking or just play around with your brand new dashboard! Next: Consuming data via APIs \u00b6","title":"Via Dashboards"},{"location":"quickstart/data-consuming/data-via-dashboards.html#consume-data-via-dashbaords","text":"In this guide you will learn how you can consume your own Datasets and your Subscriptions via the built in dashboards of your DataSpace Node. This guide is layed out for the data streams in the Quick start guide, although creating dashboards for other data streams should follow the same principle, just end up with a slightly different configuration, depending on the needs.","title":"Consume data via dashbaords"},{"location":"quickstart/data-consuming/data-via-dashboards.html#prerequisites","text":"The prerequisits for consuming data from a Subscription via the integrated Dashboards are, that a Dataset must be available at your node. If you followed the Quick Start guide, then those prerequisites should be fulfilled. This could be in one of two ways: You are Data Owner : You own some Datasets and have your own data on your node (Owner DB) You are Data Consumer : You have been granted a Subscription to a Dataset and the data is already synchronised to your node (Consumer DB)","title":"Prerequisites"},{"location":"quickstart/data-consuming/data-via-dashboards.html#node-dashboard-basics","text":"You can easily navigate to your node dashboard by clicking the \"Grafana\" menu entry in the side menu of your DataSpace Admin App. For authentication, use the same username and password that you use for authentication when accessing the DataSpace Admin App. The dashboarding system is built on the open source technology Grafana . It comes with pre-configured data sources for the in your DataSpace Node integrated Postgre SQL databases. The structure of dashboards is based on the same logic as in the interface of your DataSpace Node: Dashboards: My Datasets / Owner DB - contains owned Datasets that are connected to your DataSpace Node. Dashboards: Subscriptions / Consumer DB - contains subscriptions to Datasets from other members of your Ecosystem.","title":"Node Dashboard Basics"},{"location":"quickstart/data-consuming/data-via-dashboards.html#create-a-dashboard","text":"To visualize your data via the dashboarding system the only thing you have to do is to duplicate the pre-configured template, paste a code snippet and paste the UUID (ValueMetadataId) of your slected data stream. 1. Add a new Dashboard - Click the \"+\" button and click on the \"Dashboard\" menu item to create a new Dashboard. Then click on the settings symbol to get to the Dashboard settings of the just created Dashboard, as shown below: 2. Copy and paste the JSON Model template - First, prepare the JSON template and adjust to your unique stream ids. If you are Owner, then copy the json of the Owner DB JSON template If you are Consumer, then copy the json of the Consumer DB JSON template Copy the contents to a editor of your choice. In the editor, paste the template JSON and search for \"ReplaceWith\". There should be 4 times where you must replace the <ReplaceWith...> strings with the IDs of your data streams, which you can find as described here . Finally, copy this adjusted template JSON, paste into the settings tab \"JSON Model\" of your new dashboard and click \"Save Changes\". Optionally, you can now rename the rows / panels to your liking or just play around with your brand new dashboard!","title":"Create a Dashboard"},{"location":"quickstart/data-consuming/data-via-dashboards.html#next-consuming-data-via-apis","text":"","title":"Next: Consuming data via APIs"},{"location":"setup/overview.html","text":"Setup Overview \u00b6 This section of the documentation describes how to setup the individual components of the DataSpace Kit (DSK). The following table overview offers a quick understanding of the components which are at the core of the Tributech DataSpace Kit. Fig.1: DataSpace Kit - Components setup variants overview If you have never set up the DSK before also check out the Quick Start Guide DSK Ecoystem \u00b6 The ecosystem consists of the DataSpace Hub and is managed & operated by Tributech. If you want to own an ecosystem, please reach out to us via our contact form . DSK Node \u00b6 The DataSpace Node serves as a platform and is most likely the part of the architecture that you'll want to setup first. It can either be managed or self-hosted. Please refer to the detailed guide for the DSK Node setup to learn about its setup and the available setup options. DSK Agent \u00b6 The device that is responsible for transimitting auditable data is the DSK Agent. This device can be an Embedded device or also an Edge device. Please refer to the detailed guide for the DSK Agent setup to learn about its setup and the available setup options. DSK IoT Hub Bridge \u00b6 The DSK IoT Hub Bridge connects the Azure IoT Hub to your DSK Node. If you are using the managed variants of the setup options and/or Edge agents, then chances are that you will want to manage the devices through the Azure IoT Hub and thus connect the IoT Hub to your node. Please follow the detailed setup guide for the DSK IoT Hub Bridge to learn about how to do that.","title":"Overview"},{"location":"setup/overview.html#setup-overview","text":"This section of the documentation describes how to setup the individual components of the DataSpace Kit (DSK). The following table overview offers a quick understanding of the components which are at the core of the Tributech DataSpace Kit. Fig.1: DataSpace Kit - Components setup variants overview If you have never set up the DSK before also check out the Quick Start Guide","title":"Setup Overview"},{"location":"setup/overview.html#dsk-ecoystem","text":"The ecosystem consists of the DataSpace Hub and is managed & operated by Tributech. If you want to own an ecosystem, please reach out to us via our contact form .","title":"DSK Ecoystem"},{"location":"setup/overview.html#dsk-node","text":"The DataSpace Node serves as a platform and is most likely the part of the architecture that you'll want to setup first. It can either be managed or self-hosted. Please refer to the detailed guide for the DSK Node setup to learn about its setup and the available setup options.","title":"DSK Node"},{"location":"setup/overview.html#dsk-agent","text":"The device that is responsible for transimitting auditable data is the DSK Agent. This device can be an Embedded device or also an Edge device. Please refer to the detailed guide for the DSK Agent setup to learn about its setup and the available setup options.","title":"DSK Agent"},{"location":"setup/overview.html#dsk-iot-hub-bridge","text":"The DSK IoT Hub Bridge connects the Azure IoT Hub to your DSK Node. If you are using the managed variants of the setup options and/or Edge agents, then chances are that you will want to manage the devices through the Azure IoT Hub and thus connect the IoT Hub to your node. Please follow the detailed setup guide for the DSK IoT Hub Bridge to learn about how to do that.","title":"DSK IoT Hub Bridge"},{"location":"setup/agent/edge-setup.html","text":"DSK Agent Edge Setup \u00b6 Requirements \u00b6 The required resources to install a DSK Agent Edge are negligible in comparison to the resources required for data processing. For installation, an edge device must meet the following requirements: Software Requirements \u00b6 Type Required Preferred OS Any current Linux Distribution Ubuntu 18.04 LTS or later Runtime Docker CE + Docker-Compose Azure IoT Edge Runtime Hardware Requirements \u00b6 Type Minimum Requirement Recommend Requirement CPU 1 Core 2 Cores RAM 1 GB Ram 2 GB Ram The DSK Agent Edge itself does not actually consume this much RAM; this are the requirements for the whole system. Keep in mind that these requirements strongly depend on the actual load of the agent. These are minimum requirements for rather small to medium amounts of data. If high volumes of data have to be processed the requirements are higher. If you have any questions regarding infrastructure requirements, please talk to your contact person at Tributech or send an email to our Customer Advisory Team . Installation \u00b6 Install DSK Agent Edge on Azure IoT Edge Runtime \u00b6 The DSK Agent Edge can be installed on the Azure IoT Edge Runtime easiest through the Azure Marketplace offer . Have a look at our quick start guide to learn how to install the Agent Edge in the Azure IoT Edge Runtime. Install DSK Agent Edge on other platforms \u00b6 The Agent can be installed on any platform that supports running docker-containers. A pre-built docker-compose is ready for use - contact our Customer Advisory Team for details. Note that self-installation is currently not supported. If you want to deploy a DataSpace Agent on other platforms, please talk to your contact person at Tributech or send an email to our Customer Advisory Team . Next steps \u00b6 Check out the integration options of the Agent.","title":"Agent Edge Setup"},{"location":"setup/agent/edge-setup.html#dsk-agent-edge-setup","text":"","title":"DSK Agent Edge Setup"},{"location":"setup/agent/edge-setup.html#requirements","text":"The required resources to install a DSK Agent Edge are negligible in comparison to the resources required for data processing. For installation, an edge device must meet the following requirements:","title":"Requirements"},{"location":"setup/agent/edge-setup.html#software-requirements","text":"Type Required Preferred OS Any current Linux Distribution Ubuntu 18.04 LTS or later Runtime Docker CE + Docker-Compose Azure IoT Edge Runtime","title":"Software Requirements"},{"location":"setup/agent/edge-setup.html#hardware-requirements","text":"Type Minimum Requirement Recommend Requirement CPU 1 Core 2 Cores RAM 1 GB Ram 2 GB Ram The DSK Agent Edge itself does not actually consume this much RAM; this are the requirements for the whole system. Keep in mind that these requirements strongly depend on the actual load of the agent. These are minimum requirements for rather small to medium amounts of data. If high volumes of data have to be processed the requirements are higher. If you have any questions regarding infrastructure requirements, please talk to your contact person at Tributech or send an email to our Customer Advisory Team .","title":"Hardware Requirements"},{"location":"setup/agent/edge-setup.html#installation","text":"","title":"Installation"},{"location":"setup/agent/edge-setup.html#install-dsk-agent-edge-on-azure-iot-edge-runtime","text":"The DSK Agent Edge can be installed on the Azure IoT Edge Runtime easiest through the Azure Marketplace offer . Have a look at our quick start guide to learn how to install the Agent Edge in the Azure IoT Edge Runtime.","title":"Install DSK Agent Edge on Azure IoT Edge Runtime"},{"location":"setup/agent/edge-setup.html#install-dsk-agent-edge-on-other-platforms","text":"The Agent can be installed on any platform that supports running docker-containers. A pre-built docker-compose is ready for use - contact our Customer Advisory Team for details. Note that self-installation is currently not supported. If you want to deploy a DataSpace Agent on other platforms, please talk to your contact person at Tributech or send an email to our Customer Advisory Team .","title":"Install DSK Agent Edge on other platforms"},{"location":"setup/agent/edge-setup.html#next-steps","text":"Check out the integration options of the Agent.","title":"Next steps"},{"location":"setup/agent/setup-options.html","text":"Setup Options \u00b6 There are three variants of the DSK Agent available: DSK Agent Edge (software-based) DSK Agent Embedded (hardware-based) DSK Agent Integrated (running at the DSK Node) DSK Agent Edge \u00b6 The DSK Agent Edge is the most common Agent variant and is perfect for all software-based use-cases. It is available on the Azure Marketplace for straight-forward installation or also in the form of a ready-to-use docker-compose file. Learn how to set it up in our specific setup guide . When to choose: This option should be preferred if you have access to the controllers of the machines on the shop-floor but not to the sensors directly (e.g. Industrial IoT Gateway). Data signing: Data is signed on the shop-floor still close to the data-source (the closer to the data-source, the better for security). Integration: You can use your existing connectors, the Agent is integrated at the message bus layer. DSK Agent Embedded \u00b6 The DSK Agent Embedded reads data directly from sensors and uses a dedicated hardware security module for signing the data-streams. This is available as either a C implementation or as a dedicated hardware module in the form of our Sensor Security Module (SSM). When to choose: This option should be preferred if you have access to your system on a sensor level. Data signing: Data is signed as close as possible to the data-source (the closer to the data-source, the better for security). This option offers the highest level of security. Acquire: Contact us if you need more information or wish to acquire an SSM. DSK Agent Integrated \u00b6 The DataSpace Node comes with an integrated agent, there is no additional setup required for it. When to choose: This option should be preferred if you are looking to use a software system (for example an ERP system) as a data-source. Data signing: Data is signed as soon as it reaches the node. Integration: Data can be sent to the integrated agent through the Trust-API - have a look at node integration . API Clients for C# can be found here .","title":"Setup Options"},{"location":"setup/agent/setup-options.html#setup-options","text":"There are three variants of the DSK Agent available: DSK Agent Edge (software-based) DSK Agent Embedded (hardware-based) DSK Agent Integrated (running at the DSK Node)","title":"Setup Options"},{"location":"setup/agent/setup-options.html#dsk-agent-edge","text":"The DSK Agent Edge is the most common Agent variant and is perfect for all software-based use-cases. It is available on the Azure Marketplace for straight-forward installation or also in the form of a ready-to-use docker-compose file. Learn how to set it up in our specific setup guide . When to choose: This option should be preferred if you have access to the controllers of the machines on the shop-floor but not to the sensors directly (e.g. Industrial IoT Gateway). Data signing: Data is signed on the shop-floor still close to the data-source (the closer to the data-source, the better for security). Integration: You can use your existing connectors, the Agent is integrated at the message bus layer.","title":"DSK Agent Edge"},{"location":"setup/agent/setup-options.html#dsk-agent-embedded","text":"The DSK Agent Embedded reads data directly from sensors and uses a dedicated hardware security module for signing the data-streams. This is available as either a C implementation or as a dedicated hardware module in the form of our Sensor Security Module (SSM). When to choose: This option should be preferred if you have access to your system on a sensor level. Data signing: Data is signed as close as possible to the data-source (the closer to the data-source, the better for security). This option offers the highest level of security. Acquire: Contact us if you need more information or wish to acquire an SSM.","title":"DSK Agent Embedded"},{"location":"setup/agent/setup-options.html#dsk-agent-integrated","text":"The DataSpace Node comes with an integrated agent, there is no additional setup required for it. When to choose: This option should be preferred if you are looking to use a software system (for example an ERP system) as a data-source. Data signing: Data is signed as soon as it reaches the node. Integration: Data can be sent to the integrated agent through the Trust-API - have a look at node integration . API Clients for C# can be found here .","title":"DSK Agent Integrated"},{"location":"setup/agent/ssm-setup.html","text":"DSK Agent Embedded (SSM) Setup \u00b6","title":"DSK Agent Embedded (SSM) Setup"},{"location":"setup/agent/ssm-setup.html#dsk-agent-embedded-ssm-setup","text":"","title":"DSK Agent Embedded (SSM) Setup"},{"location":"setup/iot-hub-bridge/iot-hub-bridge.html","text":"DSK IoT Hub Bridge \u00b6 The DSK IoT Hub Bridge is the part that connects the DSK Node to the Azure IoT Hub. The IoT Hub is used for device-management and communication. The IoT Hub is also used to deploy the Edge Agent onto an Edge Device. The Edge Agent receives data from one or more data-sources and forwards it to the IoT Hub. The DSK IoT Hub Bridge forwards the data to the DSK Node. Setup \u00b6 In Azure the IoT Hub Bridge can easily be set up using an ARM Template . This template deploys a new instance of the Azure IoT Hub in combination with the DSK IoT Hub Bridge. The required steps are described in the quick-start-guide .","title":"DSK IoT Hub Bridge"},{"location":"setup/iot-hub-bridge/iot-hub-bridge.html#dsk-iot-hub-bridge","text":"The DSK IoT Hub Bridge is the part that connects the DSK Node to the Azure IoT Hub. The IoT Hub is used for device-management and communication. The IoT Hub is also used to deploy the Edge Agent onto an Edge Device. The Edge Agent receives data from one or more data-sources and forwards it to the IoT Hub. The DSK IoT Hub Bridge forwards the data to the DSK Node.","title":"DSK IoT Hub Bridge"},{"location":"setup/iot-hub-bridge/iot-hub-bridge.html#setup","text":"In Azure the IoT Hub Bridge can easily be set up using an ARM Template . This template deploys a new instance of the Azure IoT Hub in combination with the DSK IoT Hub Bridge. The required steps are described in the quick-start-guide .","title":"Setup"},{"location":"setup/node/node-requirements.html","text":"Requirements \u00b6 The requirements to install a DataSpace Node depend significantly on the amount of data being processed. As a standard infrastructure setup, we recommend that you meet the following requirements: Software Requirements \u00b6 Any up-to-date Linux distribution Recommended: Ubuntu 18.04 LTS or later Docker and Docker-Compose Hardware Requirements \u00b6 Type Minimum Requirement Recommend Requirement CPU 2 Cores 4 Cores RAM 8 GB RAM 16 GB RAM Storage System 10 GB 15 GB Storage Data depending on use-case n/a If you have any questions regarding infrastructure requirements, please talk to your contact person at Tributech or send an email to our Customer Advisory Team .","title":"Requirements"},{"location":"setup/node/node-requirements.html#requirements","text":"The requirements to install a DataSpace Node depend significantly on the amount of data being processed. As a standard infrastructure setup, we recommend that you meet the following requirements:","title":"Requirements"},{"location":"setup/node/node-requirements.html#software-requirements","text":"Any up-to-date Linux distribution Recommended: Ubuntu 18.04 LTS or later Docker and Docker-Compose","title":"Software Requirements"},{"location":"setup/node/node-requirements.html#hardware-requirements","text":"Type Minimum Requirement Recommend Requirement CPU 2 Cores 4 Cores RAM 8 GB RAM 16 GB RAM Storage System 10 GB 15 GB Storage Data depending on use-case n/a If you have any questions regarding infrastructure requirements, please talk to your contact person at Tributech or send an email to our Customer Advisory Team .","title":"Hardware Requirements"},{"location":"setup/node/setup-guide.html","text":"Setup instructions Tributech DataSpace Node \u00b6 Select a DataSpace Ecosystem \u00b6 If you want to join a DataSpace, please slelect one of the follwoing options: Existing Ecosystem - please contact the operater of this Ecosystem. New Ecosystem - please contact our Customer Advisory Team . Tributech Open DataSpace Network - follow the steps in our quick start guide . Install a DataSpace Node on Azure \u00b6 A node on Azure can be installed through the Azure Marketplace offer . See the quick start guide on how to setup a node on Azure in the public Dataspace Ecosystem. Note, that in order to deploy a node to a custom ecosystem, other then the default Azure Trial ecosystem, you will have to contact us so that we can make it available for you. Please contact our Customer Advisory Team for such cases. Install a DataSpace Node outside of Azure \u00b6 Self-installation is currently not supported for a set up of a DataSpace Node outside of Azure, please talk to your contact person at Tributech or send an email to our Customer Advisory Team . Next Steps \u00b6 Set up the Azure IoT Hub Bridge , or set up the DSK Agent Edge , or read about the integration options of the DSK.","title":"Setup Guide"},{"location":"setup/node/setup-guide.html#setup-instructions-tributech-dataspace-node","text":"","title":"Setup instructions Tributech DataSpace Node"},{"location":"setup/node/setup-guide.html#select-a-dataspace-ecosystem","text":"If you want to join a DataSpace, please slelect one of the follwoing options: Existing Ecosystem - please contact the operater of this Ecosystem. New Ecosystem - please contact our Customer Advisory Team . Tributech Open DataSpace Network - follow the steps in our quick start guide .","title":"Select a DataSpace Ecosystem"},{"location":"setup/node/setup-guide.html#install-a-dataspace-node-on-azure","text":"A node on Azure can be installed through the Azure Marketplace offer . See the quick start guide on how to setup a node on Azure in the public Dataspace Ecosystem. Note, that in order to deploy a node to a custom ecosystem, other then the default Azure Trial ecosystem, you will have to contact us so that we can make it available for you. Please contact our Customer Advisory Team for such cases.","title":"Install a DataSpace Node on Azure"},{"location":"setup/node/setup-guide.html#install-a-dataspace-node-outside-of-azure","text":"Self-installation is currently not supported for a set up of a DataSpace Node outside of Azure, please talk to your contact person at Tributech or send an email to our Customer Advisory Team .","title":"Install a DataSpace Node outside of Azure"},{"location":"setup/node/setup-guide.html#next-steps","text":"Set up the Azure IoT Hub Bridge , or set up the DSK Agent Edge , or read about the integration options of the DSK.","title":"Next Steps"},{"location":"setup/node/setup-options.html","text":"Setup Options \u00b6 This page explains the setup options for a DSK node and its prerequisites. Prerequisites \u00b6 Ecosystem \u00b6 Your new DSK Node has to join a DataSpace Ecosystem. In general, there are no limitations on how you can structure a DataSpace Ecosystem. We recommend a segmentation in 2 categories: Private DataSpace Ecosystems for the data exchange in your value chain with for example your customers, suppliers and partners. Open DataSpace Ecosystems for the data exchange with businesses, researchers and communities outside your value chain to drive innovation, try new use-cases and services or monetize data. DSK Node on Azure \u00b6 Although other setup options are also available, the easiest way to setup the DSK Node is through the Azure Marketplace. See the quick start guide on how to setup a node on Azure in the public Dataspace Ecosystem. DSK Node On Premise or other Cloud Providers \u00b6 The DSK Node can also be deployed on premise or in a virtual machine on any of the major public cloud providers. See the requirements and contact us .","title":"Setup Options"},{"location":"setup/node/setup-options.html#setup-options","text":"This page explains the setup options for a DSK node and its prerequisites.","title":"Setup Options"},{"location":"setup/node/setup-options.html#prerequisites","text":"","title":"Prerequisites"},{"location":"setup/node/setup-options.html#ecosystem","text":"Your new DSK Node has to join a DataSpace Ecosystem. In general, there are no limitations on how you can structure a DataSpace Ecosystem. We recommend a segmentation in 2 categories: Private DataSpace Ecosystems for the data exchange in your value chain with for example your customers, suppliers and partners. Open DataSpace Ecosystems for the data exchange with businesses, researchers and communities outside your value chain to drive innovation, try new use-cases and services or monetize data.","title":"Ecosystem"},{"location":"setup/node/setup-options.html#dsk-node-on-azure","text":"Although other setup options are also available, the easiest way to setup the DSK Node is through the Azure Marketplace. See the quick start guide on how to setup a node on Azure in the public Dataspace Ecosystem.","title":"DSK Node on Azure"},{"location":"setup/node/setup-options.html#dsk-node-on-premise-or-other-cloud-providers","text":"The DSK Node can also be deployed on premise or in a virtual machine on any of the major public cloud providers. See the requirements and contact us .","title":"DSK Node On Premise or other Cloud Providers"}]}